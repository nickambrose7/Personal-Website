---
title: A Deep Dive Into my Skills
date: 2023/12/21
description: Learn more about how I aquired  my skills.
tag: personal
author: Nick Ambrose
---


## Deep Dive Into My Tech Toolkit: More Than Just a Resume

Before you wonder, no, this isn't an exercise in self-adoration. I'm not here to sing endless praises about my skill set. Instead, I want to take you on a journey through the technologies I've worked with, offering an insider's view on each one. It's not just about listing skills; it's about sharing experiences and insights that might just resonate with you or help you on your own tech journey. To potential employers reading- I hope this post gives you a realistic sense of how familar I am with each of these technologies.

## FastAPI: Speed and Efficiency in Web Development
FastAPI was the backbone of my Workout API project. Its rapid data processing capabilities and easy integration with asynchronous Python code made it an ideal choice for handling real-time workout data and user requests efficiently. A standout feature of FastAPI was its automatic generation of interactive API documentation, making the testing process easy. Moreover, the integration of Pydantic for data validation and serialization was crucial. It ensured type safety, minimized common errors, and allowed me to concentrate on the core business logic, thereby making the API robust and maintainable. 

## SQLAlchemy: Navigating the ORM World
SQLAlchemy was a great introduction for me into the world of database connectivity libraries. I really appreciated its dual nature, offering both a Core and an ORM layer. This flexibility allowed me to toggle between using an ORM for some tasks and writing raw SQL for others, depending on my project requirements or simply my mood that day. Learning SQLAlchemy was an interesting journey, particularly in understanding how to safeguard against injection attacks. While I'm not claiming to be an expert in SQLAlchemy just yet, especially with its intricate ORM layer, I'm grateful for the solid foundation it's provided me in database interactions.

## Alembic: Mastering Database Migrations
Alongside my journey with SQLAlchemy, I also got my hands on Alembic for managing database migrations and versions. It was an awesome tool for tracking and applying changes to the database schema over time. While I did make use of its auto-generation capabilities for migration scripts, I often found myself tweaking these scripts manually in order to get things to work. This experience with Alembic not only reinforced my understanding of database version control but also highlighted the importance of having a direct hand in how databases evolve with an application.


## Faker: Realistic Data Generation
Faker became an indispensable tool in my arsenal for generating realistic, yet fake, data for testing. Its ability to create a wide range of data, from simple names to complex credit card details, made database population a breeze. What struck me most during the process of using Faker was a crucial insight: building a database for small-scale data is one thing, but engineering it to efficiently handle a large amount of data is an entirely different ball game. The significance of efficient database design, particularly indexing, became crystal clear. This experience was a true eye-opener about database performance and optimization.

## Supabase: Embracing the Backend as a Service
Supabase was a game-changer for hosting our Workout API's Postgres database. Its built-in SQL editor simplified database management, and its user-friendly interface was perfect for juggling between local, staging, and production databases. 

## Vercel: Effortless Deployment
I've really taken a liking to Vercel. It's known for hosting static sites, but it worked fine for deploying my Workout API too. The platform makes life easier by allowing for instant deployment, automatic scaling, and minimal supervision, all with next to no configuration hassle. I'm a big fan of any tool that makes my life easier, and Vercel is definitly one of those.

## React: Building User Interfaces with Ease
Since diving into React, I've learned a lot about its core elements: Props for passing data, Hooks for adding state to functional components, Routing for seamless page navigation, and Event Handling for interactive UIs. These essentials, combined with my knack for designing reusable components and managing state, have empowered me to craft dynamic and responsive interfaces. I especially enjoy blending React's capabilities with Bootstrap's styling to create visually appealing layouts. Each React project is a journey of continuous learning, but I'm thrilled with the skills I've developed so far in this versatile framework.

## Express.js: Simplifying Backend Development
Express.js has been great for backend development. Its minimalist approach, combined with powerful routing capabilities, greatly eased the process of building RESTful APIs.

## Axios: Streamlining HTTP Requests
Axios, with its promise-based structure, has been instrumental in handling HTTP requests. Its ease of use for making asynchronous calls and handling responses efficiently made it an integral part of my toolkit.

## Postman: Testing APIs Made Simple
Postman has been indispensable for API testing. Its user-friendly interface and comprehensive testing capabilities allowed me to thoroughly test and debug APIs, ensuring reliability and performance.

## MongoDB: Navigating NoSQL Databases
Exploring MongoDB introduced me to the world of NoSQL databases. Its flexible schema design and scalability were key in managing datasets effectively, my main experience came in using mongoDB cloud atlas which provided an easy way to deploy, manage, and scale MongoDB databases in the cloud.


## CI with GitHub Actions: Automating Workflows
GitHub Actions was my first exposure to Continuous Integration (CI). The configuration was straightforward, offering peace of mind as each code push automatically triggered build and tesing processes. This not only saved time but also ensured that any integration issues were caught early, significantly improving code quality and efficiency in the development cycle.

## Modeling: ER & UML Diagrams
My experience with modeling using ER (Entity-Relationship) and UML (Unified Modeling Language) diagrams sharpened my ability to design and visualize system architectures. ER diagrams helped me conceptualize database relationships, while UML diagrams were invaluable for outlining class structures and interactions in complex software systems. These modeling tools are crucial for planning and communicating system designs effectively. They also taught me how helpful it is to have a well-thoughtout plan before writing a single line of code.


## Agile Development: Embracing Flexibility and Collaboration
Agile Development profoundly shaped my approach to project management. Embracing Agile's iterative process, I learned to value adaptability and teamwork in rapidly changing environments. This methodology highlighted the importance of regular feedback, continuous improvement, and effective communication within a team. It wasn't just about following a set of practices, but more about cultivating a mindset geared towards collaborative problem-solving and customer-focused product development. Agile has been instrumental in my projects, ensuring that the end product truly aligns with user needs and expectations.

## NumPy & Pandas: Data Analysis Made Easy
During the development of my Divorce Predictor Machine Learning Model, I gained practical experience in harnessing the capabilities of NumPy and Pandas. My journey with Pandas involved extensive interaction with its DataFrame structure, which proved to be an invaluable tool in importing, cleaning, and transforming my dataset with ease and efficiency. This experience allowed me to appreciate the intuitiveness and flexibility of Pandas in handling complex data operations. Simultaneously, I leveraged the robustness of NumPy's array object, delving into its comprehensive suite of functionalities including array manipulation, mathematical operations, aggregation techniques, and random number generation. These features were particularly useful in the development of my K-Nearest Neighbors (KNN) and K-Means algorithms. This hands-on experience with both libraries significantly enhanced my skill set in data analysis and algorithm implementation, solidifying my understanding of these powerful tools in practical machine learning contexts.

## Lessons Learned: Beyond Technology
- Reflect on the non-technical lessons from working with these technologies.
- Emphasize the importance of documentation and managing concurrent requests.
- Offer a piece of advice or a key takeaway for your readers.

## Conclusion
- Wrap up by emphasizing the importance of practical experience over theoretical knowledge.
- Encourage readers to explore and experiment with technologies.
- Invite feedback and further discussion in the comments.
